{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8686f-f05b-410e-b9f3-44ef21109eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc083fc-a9f5-4c9b-8ce2-d75f3bd44d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants representing labels\n",
    "Bankruptcy = 1\n",
    "Non_Bankruptcy = 0\n",
    "\n",
    "# Load images\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(folder, filename)\n",
    "            image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "            image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            images.append(image_array)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "bankruptcy_images, bankruptcy_labels = load_images_from_folder('FS0/Bankruptcy', Bankruptcy)\n",
    "non_bankruptcy_images, non_bankruptcy_labels = load_images_from_folder('FS0/NonBankruptcy', Non_Bankruptcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25d676-b2b8-409f-8eb5-819559ebc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for 1:2 ratio\n",
    "# Ensure there are twice as many non-bankruptcy images as bankruptcy images\n",
    "min_size = len(bankruptcy_images)  # Number of bankruptcy images\n",
    "non_bankruptcy_images = non_bankruptcy_images[:2 * min_size]  # Double the number of bankruptcy images\n",
    "non_bankruptcy_labels = [Non_Bankruptcy] * len(non_bankruptcy_images)  # Adjust labels accordingly\n",
    "\n",
    "# Combine and shuffle the dataset\n",
    "images = np.array(bankruptcy_images + non_bankruptcy_images)\n",
    "labels = np.array(bankruptcy_labels + non_bankruptcy_labels)\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd26c7a-8401-454a-b8e7-1ebf32bd3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e71d4-6d4f-47dd-998c-e83f7c6a6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers with dropout for regularization\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output and add dense layers with dropout\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbafcc-00c2-41a8-893c-4cdeb044726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "avg_accuracy = []\n",
    "avg_recall = []\n",
    "avg_precision = []\n",
    "avg_f1 = []\n",
    "avg_roc_auc = []\n",
    "avg_kappa = []\n",
    "avg_type_ii_error = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skfold.split(X_train, y_train), 1):  \n",
    "    # Split the data into train and test sets for this fold\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    \n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test_fold)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_test_fold, y_pred_binary)\n",
    "    recall = recall_score(y_test_fold, y_pred_binary)\n",
    "    precision = precision_score(y_test_fold, y_pred_binary)\n",
    "    f1 = f1_score(y_test_fold, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_test_fold, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test_fold, y_pred_binary)\n",
    "    type_ii_error = 1 - recall\n",
    "    \n",
    "    print(f'Fold {fold}:')\n",
    "    print(f'  Accuracy: {accuracy}')\n",
    "    print(f'  Recall: {recall}')\n",
    "    print(f'  Precision: {precision}')\n",
    "    print(f'  F1 Score: {f1}')\n",
    "    print(f'  ROC AUC: {roc_auc}')\n",
    "    print(f'  Kappa: {kappa}')\n",
    "    print(f'  type_ii_error: {type_ii_error}')\n",
    "    print()\n",
    "\n",
    "    # Append metrics for this fold to the lists\n",
    "    avg_accuracy.append(accuracy)\n",
    "    avg_recall.append(recall)\n",
    "    avg_precision.append(precision)\n",
    "    avg_f1.append(f1)\n",
    "    avg_roc_auc.append(roc_auc)\n",
    "    avg_kappa.append(kappa)\n",
    "    avg_type_ii_error.append(type_ii_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a674ba2-b73e-48c4-988b-6008f1945652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics\n",
    "avg_accuracy = np.mean(avg_accuracy)\n",
    "avg_recall = np.mean(avg_recall)\n",
    "avg_precision = np.mean(avg_precision)\n",
    "avg_f1 = np.mean(avg_f1)\n",
    "avg_roc_auc = np.mean(avg_roc_auc)\n",
    "avg_kappa = np.mean(avg_kappa)\n",
    "avg_type_ii_error = np.mean(type_ii_error)\n",
    "\n",
    "# Print average metrics\n",
    "print(f'Average Accuracy: {avg_accuracy}')\n",
    "print(f'Average Recall: {avg_recall}')\n",
    "print(f'Average Precision: {avg_precision}')\n",
    "print(f'Average F1 Score: {avg_f1}')\n",
    "print(f'Average ROC AUC: {avg_roc_auc}')\n",
    "print(f'Average Kappa: {avg_kappa}')\n",
    "print(f'Average Type II Error: {avg_type_ii_error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d36a08-b8f8-44f9-b4cd-10e8f14ba525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('CNNFS_balanced_1_2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
